{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFYc0XqbsrAzV3vif8ladm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TusharGen/LearnzyLandingPage/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install systemtools wheel requests bs4 numpy pandas pybind11 nltk networkx gensim scikit-learn scipy pylcs rank_bm25\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUD-gRZJcImG",
        "outputId": "8c46e245-4b7d-4864-d43b-9802ad580216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: systemtools in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.40.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.10/dist-packages (2.10.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: pylcs in /usr/local/lib/python3.10/dist-packages (0.0.8)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ibTtA-LSWFB"
      },
      "outputs": [],
      "source": [
        "# For collecting data from URL\n",
        "import requests, bs4\n",
        "\n",
        "# To collect text from a file\n",
        "from os.path import abspath\n",
        "\n",
        "# Basic libraries\n",
        "import numpy as np, pandas as pd, re\n",
        "from math import log\n",
        "\n",
        "# Summarization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "from gensim import corpora\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import spatial\n",
        "from pylcs import lcs\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from gensim.summarization import bm25\n",
        "from rank_bm25 import BM25Plus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the essential NLTK modules\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHCvXN1VSojO",
        "outputId": "812f42ba-8093-400b-a2ae-9995d89420d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def take_url():\n",
        "    url = input(\"Enter the url :\")\n",
        "    response = requests.get(url)\n",
        "    soup = bs4.BeautifulSoup(response.content, \"html.parser\")\n",
        "    data = soup.text\n",
        "    #print(data)\n",
        "    return data"
      ],
      "metadata": {
        "id": "K8T2z7DkSreR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def file_input():\n",
        "    file_location = input(\"Enter the path of file :\")\n",
        "    print(f\"\\nReading the file: {file_location}\")\n",
        "    with open(abspath(file_location), encoding=\"utf8\") as f:\n",
        "        data = \"\\n\".join(f.readlines())\n",
        "    return data"
      ],
      "metadata": {
        "id": "25lvuHlASuSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def take_input():\n",
        "    print(\"Enter the method for text input :\")\n",
        "    print(\" 1. From URL \\n 2. From text file \\n 3. Direct input \\n Enter choice :\")\n",
        "\n",
        "    try:\n",
        "        choice = int(input())\n",
        "    except:\n",
        "        return -1\n",
        "\n",
        "    if(choice == 1):\n",
        "        return take_url()\n",
        "\n",
        "    elif(choice == 2):\n",
        "        return file_input()\n",
        "\n",
        "    elif(choice == 3):\n",
        "        text = input(\"Enter the text : \")\n",
        "        return text\n",
        "\n",
        "    else:\n",
        "        return -1"
      ],
      "metadata": {
        "id": "F9coyEEbSwT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(word):\n",
        "\n",
        "    # Map POS tag to first character lemmatize() accepts\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "cpAPZBLFSzqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_original(text_data):\n",
        "\n",
        "    sim = np.zeros([len(text_data), len(text_data)]) # Initialization\n",
        "    for i, sentence_1 in enumerate(text_data):\n",
        "        for j, sentence_2 in enumerate(text_data):\n",
        "            sent_1 = set(sentence_1) # Unique words\n",
        "            sent_2 = set(sentence_2)\n",
        "\n",
        "            if(i == j):\n",
        "                sim[i][j] = 0\n",
        "            else:\n",
        "                common = float(len(list(sent_1 & sent_2)))\n",
        "                if(len(sentence_1) and len(sentence_2) > 1):\n",
        "                    denominator = float(log(len(sentence_1)) + log(len(sentence_2)))\n",
        "                else:\n",
        "                    denominator = 1.0\n",
        "\n",
        "                sim[i][j] = common / denominator\n",
        "    return sim"
      ],
      "metadata": {
        "id": "1OqTRWLNS2FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_bm(text_data):\n",
        "\n",
        "    dictionary = corpora.Dictionary(text_data) # BAG_OF_WORDS MODEL\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "    # bm25_obj = bm25.BM25(corpus) #object\n",
        "    bm25_obj = BM25Plus(corpus)\n",
        "\n",
        "    similarity = []\n",
        "\n",
        "    for i, sentence in enumerate(text_data):\n",
        "        query = dictionary.doc2bow(sentence)\n",
        "        score = bm25_obj.get_scores(query)\n",
        "        similarity.append(score)\n",
        "\n",
        "    sim = np.array(similarity)\n",
        "    return sim"
      ],
      "metadata": {
        "id": "Bm9dvA-1S4zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(string, ratio = 0.2):\n",
        "\n",
        "    # Tokenization\n",
        "    sentences = sent_tokenize(string)\n",
        "    sentences_clean = [re.sub(r'[^\\w\\s]','',sentence.lower()) for sentence in sentences]\n",
        "\n",
        "    # Stop words removal\n",
        "    stop_words = stopwords.words('english')\n",
        "    sentence_tokens = [[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]\n",
        "\n",
        "    # POS Tagging and Lemmatization\n",
        "    text_data = []\n",
        "    lemmatizer = WordNetLemmatizer() #object\n",
        "    count = 0\n",
        "\n",
        "    for sentence in sentence_tokens:\n",
        "        sample_list = [word for word in sentence if word]\n",
        "        tags = pos_tag(sample_list)\n",
        "        line = []\n",
        "        for word, tag in tags:\n",
        "            count += 1\n",
        "            line.append(lemmatizer.lemmatize(word, pos = get_wordnet_pos(tag)))\n",
        "        text_data.append(line)\n",
        "\n",
        "    # Similarity Matrix\n",
        "    sim_a = similarity_original(text_data)\n",
        "    sim_d = similarity_bm(text_data)\n",
        "\n",
        "    # Normalization\n",
        "    sim_a = sim_a / sim_a.max()\n",
        "    sim_d = sim_d / sim_d.max()\n",
        "\n",
        "    # Combination\n",
        "    similarity_matrix = (sim_a + sim_d)\n",
        "\n",
        "    # Page Rank\n",
        "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "    scores = nx.pagerank(nx_graph, max_iter = 600)\n",
        "\n",
        "    # Best sentences\n",
        "    top_sentence = {sentence:scores[index] for index,sentence in enumerate(sentences)}\n",
        "    number = int(len(sentence_tokens)*(ratio))\n",
        "    top = dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:number])\n",
        "    text_list = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        if sent in top.keys():\n",
        "            text_list.append(sent)\n",
        "\n",
        "    summary = \"\\n\".join(text_list)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "zOYy9plwS7S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    text = take_input()\n",
        "    if (text == -1):\n",
        "        print(\"Wrong Input\")\n",
        "    else:\n",
        "        summary = summarize(text)\n",
        "        print(\"\\nSummary :\\n\", summary)"
      ],
      "metadata": {
        "id": "ywYy5UAwS-wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfi7FBiYTBJQ",
        "outputId": "647df163-920a-4cb8-ade4-bd574d873e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the method for text input :\n",
            " 1. From URL \n",
            " 2. From text file \n",
            " 3. Direct input \n",
            " Enter choice :\n",
            "1\n",
            "Enter the url :https://en.wikipedia.org/wiki/Main_Page\n",
            "\n",
            "Summary :\n",
            " Union troops had been using Helena, Arkansas, as a base of operations since July 1862.\n",
            "(Full article...)\n",
            "\n",
            "\n",
            "Recently featured: \n",
            "Mauritius sheldgoose\n",
            "Thomas A. Spragens\n",
            "Title (EP)\n",
            "\n",
            "\n",
            "Archive\n",
            "By email\n",
            "More featured articles\n",
            "About\n",
            "\n",
            "Did you know ...\n",
            "\n",
            "\n",
            "\n",
            "First Memorial Presbyterian Church\n",
            "\n",
            "... that the highest point on Blackwell Street is the bell tower of the First Memorial Presbyterian Church (pictured)?\n",
            "Archive\n",
            "Start a new article\n",
            "Nominate an article\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the news\n",
            "\n",
            "\n",
            "Jair Bolsonaro\n",
            "\n",
            "In Brazil, the Superior Electoral Court bars former president Jair Bolsonaro (pictured) from running for political office until 2030 for abuse of power before the 2022 general election.\n",
            "Ongoing: \n",
            "Russian invasion of Ukraine\n",
            "Sudan conflict\n",
            "Recent deaths: \n",
            "Stephen Owen\n",
            "Corazon Nuñez Malanyaon\n",
            "Dilano van 't Hoff\n",
            "Marvin Kitman\n",
            "Sue Johanson\n",
            "Robert Sherman\n",
            "\n",
            "Nominate an article\n",
            "\n",
            "On this day\n",
            "\n",
            "July 4: Republic Day in the Philippines (1946); Independence Day in the United States (1776)\n",
            "\n",
            "\n",
            "\n",
            "The Brazilian cruiser Bahia\n",
            "\n",
            "1054 – Chinese astronomers recorded the sudden appearance of a \"guest star\", later identified as the supernova that created the Crab Nebula.\n",
            "1643 – First English Civil War: Royalist forces defeated the Parliamentarians at the Battle of Burton Bridge, securing a crossing of the River Trent for a convoy of supplies travelling with Queen Henrietta Maria.\n",
            "1995)Barthélemy Mukenge (d. 2018)\n",
            "\n",
            "More anniversaries: \n",
            "July 3\n",
            "July 4\n",
            "July 5\n",
            "\n",
            "\n",
            "Archive\n",
            "By email\n",
            "List of days of the year\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Today's featured picture\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The Large Hadron Collider (LHC) is the world's largest and highest-energy particle collider.\n",
            "Photograph credit: Maximilien Brice\n",
            "\n",
            "Recently featured: \n",
            "K. K. Usha\n",
            "Chatham albatross\n",
            "Alcobaça Monastery\n",
            "\n",
            "\n",
            "Archive\n",
            "More featured pictures\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Other areas of Wikipedia\n",
            "\n",
            "Community portal – The central hub for editors, with resources, links, tasks, and announcements.\n",
            "Wikipedia's sister projects\n",
            "\n",
            "Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CommonsFree media repository\n",
            "\n",
            "\n",
            "\n",
            "MediaWikiWiki software development\n",
            "\n",
            "\n",
            "\n",
            "Meta-WikiWikimedia project coordination\n",
            "\n",
            "\n",
            "\n",
            "WikibooksFree textbooks and manuals\n",
            "\n",
            "\n",
            "\n",
            "WikidataFree knowledge base\n",
            "\n",
            "\n",
            "\n",
            "WikinewsFree-content news\n",
            "\n",
            "\n",
            "\n",
            "WikiquoteCollection of quotations\n",
            "\n",
            "\n",
            "\n",
            "WikisourceFree-content library\n",
            "\n",
            "\n",
            "\n",
            "WikispeciesDirectory of species\n",
            "\n",
            "\n",
            "\n",
            "WikiversityFree learning tools\n",
            "\n",
            "\n",
            "\n",
            "WikivoyageFree travel guide\n",
            "\n",
            "\n",
            "\n",
            "WiktionaryDictionary and thesaurus\n",
            "\n",
            "\n",
            "\n",
            "Wikipedia languages\n",
            "\n",
            "\n",
            "This Wikipedia is written in English.\n",
            "1,000,000+ articles\n",
            "\n",
            "\n",
            "\n",
            "العربية\n",
            "Deutsch\n",
            "Español\n",
            "Français\n",
            "Italiano\n",
            "Nederlands\n",
            "日本語\n",
            "Polski\n",
            "Português\n",
            "Русский\n",
            "Svenska\n",
            "Українська\n",
            "Tiếng Việt\n",
            "中文\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "250,000+ articles\n",
            "\n",
            "\n",
            "\n",
            "Bahasa Indonesia\n",
            "Bahasa Melayu\n",
            "Bân-lâm-gú\n",
            "Български\n",
            "Català\n",
            "Čeština\n",
            "Dansk\n",
            "Esperanto\n",
            "Euskara\n",
            "فارسی‎\n",
            "עברית\n",
            "한국어\n",
            "Magyar\n",
            "Norsk bokmål\n",
            "Română\n",
            "Srpski\n",
            "Srpskohrvatski\n",
            "Suomi\n",
            "Türkçe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "50,000+ articles\n",
            "\n",
            "\n",
            "\n",
            "Asturianu\n",
            "বাংলা\n",
            "Bosanski\n",
            "Eesti\n",
            "Ελληνικά\n",
            "Simple English\n",
            "Frysk\n",
            "Gaeilge\n",
            "Galego\n",
            "Hrvatski\n",
            "ქართული\n",
            "Latviešu\n",
            "Lietuvių\n",
            "മലയാളം\n",
            "Македонски\n",
            "Norsk nynorsk\n",
            "ਪੰਜਾਬੀ\n",
            "Shqip\n",
            "Slovenčina\n",
            "Slovenščina\n",
            "ไทย\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Retrieved from \"https://en.wikipedia.org/w/index.php?title=Main_Page&oldid=1114291180\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "47 languages\n",
            "\n",
            "\n",
            "\n",
            "العربيةবাংলাБългарскиBosanskiCatalàČeštinaDanskDeutschEestiΕλληνικάEspañolEsperantoEuskaraفارسیFrançaisGalego한국어HrvatskiBahasa IndonesiaItalianoעבריתქართულიLatviešuLietuviųMagyarМакедонскиBahasa MelayuNederlands日本語Norsk bokmålNorsk nynorskPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaСрпски / srpskiSrpskohrvatski / српскохрватскиSuomiSvenskaไทยTürkçeУкраїнськаTiếng Việt中文\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " This page was last edited on 5 October 2022, at 19:27 (UTC).\n"
          ]
        }
      ]
    }
  ]
}